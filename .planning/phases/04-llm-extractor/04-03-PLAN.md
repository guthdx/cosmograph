---
phase: 04-llm-extractor
plan: 03
type: execute
wave: 3
depends_on: ["04-02"]
files_modified:
  - src/cosmograph/cli.py
  - src/cosmograph/extractors/__init__.py
  - tests/test_llm_extractor.py
autonomous: true

must_haves:
  truths:
    - "CLI accepts -e llm option"
    - "CLI shows approval prompt for LLM extraction"
    - "Tests pass without making real API calls"
    - "Mocked tests verify extraction logic"
    - "Import error is graceful when anthropic not installed"
  artifacts:
    - path: "src/cosmograph/cli.py"
      provides: "LLM extractor option in CLI"
      contains: "llm"
    - path: "tests/test_llm_extractor.py"
      provides: "Mocked tests for LLM extractor"
      min_lines: 100
  key_links:
    - from: "src/cosmograph/cli.py"
      to: "src/cosmograph/extractors/llm.py"
      via: "extractor selection"
      pattern: "LlmExtractor"
    - from: "tests/test_llm_extractor.py"
      to: "src/cosmograph/extractors/llm.py"
      via: "pytest mocks"
      pattern: "unittest\\.mock"
---

<objective>
Add CLI integration for LLM extractor and comprehensive tests with mocked API responses.

Purpose: Complete the feature so operators can use `-e llm` from command line, and ensure code quality with tests that don't hit the real API.

Output: Working CLI option and passing test suite.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/milestones/v0.2.0/ROADMAP.md
@.planning/phases/04-llm-extractor/04-RESEARCH.md

@src/cosmograph/cli.py
@src/cosmograph/extractors/__init__.py
@src/cosmograph/extractors/llm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add CLI integration for LLM extractor</name>
  <files>
    src/cosmograph/cli.py
    src/cosmograph/extractors/__init__.py
  </files>
  <action>
1. **Update extractors/__init__.py:**
   - Add conditional import for LlmExtractor:
     ```python
     try:
         from .llm import LlmExtractor, OperatorDeclinedError, HAS_ANTHROPIC
     except ImportError:
         HAS_ANTHROPIC = False
         LlmExtractor = None  # type: ignore
         OperatorDeclinedError = Exception  # type: ignore
     ```
   - Add to __all__: "LlmExtractor", "OperatorDeclinedError", "HAS_ANTHROPIC"

2. **Update cli.py _get_extractor function:**
   - Add "llm" case:
     ```python
     elif extractor_type == "llm":
         from .extractors import LlmExtractor, HAS_ANTHROPIC
         if not HAS_ANTHROPIC:
             console.print("[red]Error:[/red] LLM extractor requires anthropic package.")
             console.print("[dim]Install with: pip install cosmograph[llm][/dim]")
             raise typer.Exit(1)
         return LlmExtractor(graph, interactive=True)
     ```

3. **Update CLI help text:**
   - Change extractor help: "Extractor type: auto, legal, text, generic, pdf, llm"

4. **Handle OperatorDeclinedError in generate command:**
   - Import OperatorDeclinedError from .extractors
   - Wrap extraction in try/except:
     ```python
     try:
         extractor_instance.extract(filepath)
     except OperatorDeclinedError:
         console.print("[yellow]LLM extraction declined by operator[/yellow]")
         raise typer.Exit(0)  # Not an error, just a user decision
     except Exception as e:
         console.print(f"[yellow]Warning:[/yellow] Failed to process {filepath.name}: {e}")
     ```

5. **Add --no-confirm flag for non-interactive LLM usage:**
   ```python
   no_confirm: bool = typer.Option(
       False,
       "--no-confirm",
       help="Skip confirmation prompt for LLM extraction (use with caution)"
   )
   ```
   Pass to LlmExtractor: `LlmExtractor(graph, interactive=not no_confirm)`
  </action>
  <verify>
    - `ruff check src/cosmograph/cli.py` passes
    - `cosmograph generate --help` shows "llm" in extractor options
    - `cosmograph generate test.txt -e llm` prompts for confirmation (with ANTHROPIC_API_KEY set)
    - Without anthropic installed, shows helpful error message
  </verify>
  <done>
    CLI accepts `-e llm` option with interactive approval gate.
    --no-confirm flag allows non-interactive usage.
    Graceful error when anthropic not installed.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create comprehensive tests with mocked API</name>
  <files>
    tests/test_llm_extractor.py
  </files>
  <action>
Create `tests/test_llm_extractor.py` with mocked API tests:

1. **Test fixture setup:**
   ```python
   import pytest
   from unittest.mock import Mock, patch, MagicMock
   from pathlib import Path

   # Skip all tests if anthropic not installed
   pytest.importorskip("anthropic")

   from cosmograph.extractors.llm import (
       LlmExtractor,
       ExtractionResult,
       ExtractedEntity,
       ExtractedRelationship,
       OperatorDeclinedError,
   )
   from cosmograph.models import Graph
   ```

2. **Mock fixtures:**
   ```python
   @pytest.fixture
   def mock_anthropic_client():
       """Mock Anthropic client for testing."""
       with patch("cosmograph.extractors.llm.anthropic") as mock:
           client_instance = MagicMock()
           mock.Anthropic.return_value = client_instance
           yield client_instance

   @pytest.fixture
   def sample_extraction_result():
       """Sample extraction result for mocking."""
       return ExtractionResult(
           entities=[
               ExtractedEntity(id="tribal_council", name="Tribal Council", category="organization"),
               ExtractedEntity(id="chairman", name="Chairman", category="person"),
           ],
           relationships=[
               ExtractedRelationship(source_id="chairman", target_id="tribal_council", relationship_type="leads"),
           ],
       )
   ```

3. **Test classes:**

   **TestLlmExtractorInit:**
   - test_init_creates_graph
   - test_init_accepts_existing_graph
   - test_init_default_model
   - test_init_custom_model
   - test_init_interactive_default_true

   **TestDocumentChunking:**
   - test_small_document_single_chunk
   - test_large_document_multiple_chunks
   - test_chunk_overlap_exists
   - test_chunks_split_at_paragraphs

   **TestTokenEstimation:**
   - test_estimate_tokens_calls_api (mock count_tokens)
   - test_estimate_returns_cost (verify calculation)
   - test_cost_calculation_sonnet (verify $3/$15 pricing)
   - test_cost_calculation_haiku (verify $1/$5 pricing)

   **TestApprovalGate:**
   - test_non_interactive_auto_approves
   - test_interactive_approval_yes (mock input)
   - test_interactive_approval_no (mock input)
   - test_declined_raises_error

   **TestExtraction:**
   - test_extract_parses_response (mock API, verify Graph populated)
   - test_extract_handles_empty_result
   - test_extract_deduplicates_entities
   - test_extract_creates_edges

   **TestRateLimiting:**
   - test_retries_on_rate_limit (mock RateLimitError)
   - test_gives_up_after_max_retries

4. **Integration test (still mocked):**
   ```python
   def test_full_extraction_flow(mock_anthropic_client, sample_extraction_result, tmp_path):
       """Test full extraction from file to graph."""
       # Create test file
       test_file = tmp_path / "test.txt"
       test_file.write_text("The Tribal Council governs the reservation...")

       # Mock API responses
       mock_anthropic_client.messages.count_tokens.return_value = Mock(input_tokens=100)
       mock_anthropic_client.beta.messages.parse.return_value = Mock(
           parsed_output=sample_extraction_result,
           stop_reason="end_turn"
       )

       # Run extraction
       extractor = LlmExtractor(interactive=False)
       graph = extractor.extract(test_file)

       # Verify results
       assert len(graph.nodes) == 2
       assert "tribal_council" in graph.nodes or "Tribal Council" in str(graph.nodes)
   ```

5. **Test file count: aim for 15-20 tests covering all critical paths**
  </action>
  <verify>
    - `pytest tests/test_llm_extractor.py -v` passes (all tests green)
    - No real API calls made (verify with `pytest --capture=no`)
    - Coverage on llm.py: `pytest tests/test_llm_extractor.py --cov=cosmograph.extractors.llm`
  </verify>
  <done>
    15-20 tests covering: init, chunking, token estimation, approval gate, extraction, rate limiting.
    All tests use mocked API - no real calls to Anthropic.
    Tests skip gracefully if anthropic not installed.
  </done>
</task>

<task type="auto">
  <name>Task 3: Verify full integration and update exports</name>
  <files>
    src/cosmograph/extractors/__init__.py
  </files>
  <action>
1. **Ensure clean exports in __init__.py:**
   - Verify __all__ includes all public items
   - Ensure conditional imports don't break when anthropic missing

2. **Run full test suite:**
   ```bash
   pytest tests/ -v
   ```
   Ensure all existing tests still pass (no regressions).

3. **Run linting on entire codebase:**
   ```bash
   ruff check src/cosmograph/
   mypy src/cosmograph/
   ```

4. **Verify CLI end-to-end (if ANTHROPIC_API_KEY available):**
   - Create a small test document
   - Run: `cosmograph generate test.txt -e llm --no-confirm -o /tmp/llm-test`
   - If no API key, verify the approval prompt appears correctly

5. **Document any type: ignore comments:**
   - If mypy requires ignores for SDK typing issues, add them with comments explaining why
  </action>
  <verify>
    - `pytest tests/ -v` - all tests pass
    - `ruff check src/cosmograph/` - no errors
    - `mypy src/cosmograph/` - passes or only expected SDK issues
    - `from cosmograph.extractors import LlmExtractor` works
  </verify>
  <done>
    Full test suite passes with no regressions.
    LlmExtractor properly exported from extractors package.
    CLI -e llm option works as expected.
  </done>
</task>

</tasks>

<verification>
After all tasks:
1. `pytest tests/ -v` passes (all tests including new LLM tests)
2. `ruff check src/cosmograph/` passes
3. `cosmograph generate --help` shows llm option
4. `python -c "from cosmograph.extractors import LlmExtractor"` works
5. Test count: 15-20 new tests for LLM extractor
</verification>

<success_criteria>
- CLI accepts `-e llm` with helpful error if anthropic not installed
- --no-confirm flag bypasses interactive prompt
- OperatorDeclinedError handled gracefully (exit 0, not error)
- 15-20 tests with mocked API responses
- Tests skip cleanly if anthropic not installed
- All existing tests still pass (no regressions)
- Coverage on llm.py >80%
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-extractor/04-03-SUMMARY.md`
</output>
