---
phase: 04-llm-extractor
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/cosmograph/extractors/llm.py
autonomous: true

must_haves:
  truths:
    - "Token count is estimated before API call"
    - "Cost estimate is displayed to operator"
    - "Operator must confirm before document is sent to API"
    - "Document hash is logged for audit (not content)"
    - "Non-interactive mode can bypass confirmation"
  artifacts:
    - path: "src/cosmograph/extractors/llm.py"
      provides: "Token estimation, cost calculation, approval gate"
      contains: "count_tokens"
  key_links:
    - from: "src/cosmograph/extractors/llm.py"
      to: "client.messages.count_tokens"
      via: "Token counting API call"
      pattern: "count_tokens"
    - from: "src/cosmograph/extractors/llm.py"
      to: "rich.console.Console"
      via: "User approval prompt"
      pattern: "console\\.input"
---

<objective>
Add token estimation, cost display, and approval gate to LlmExtractor for data sovereignty compliance.

Purpose: Operators must see estimated costs and explicitly approve before any document content is sent to Claude API. This is a core data sovereignty requirement.

Output: LlmExtractor with approval gate that shows token estimate and cost before extraction.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/milestones/v0.2.0/ROADMAP.md
@.planning/phases/04-llm-extractor/04-RESEARCH.md

@src/cosmograph/extractors/llm.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add token counting and cost estimation</name>
  <files>
    src/cosmograph/extractors/llm.py
  </files>
  <action>
Add token counting and cost estimation methods to LlmExtractor:

1. **Pricing constants (class level):**
   ```python
   # Sonnet 4.5 pricing per million tokens
   _PRICING = {
       "claude-sonnet-4-5": {"input": 3.00, "output": 15.00},
       "claude-haiku-4-5": {"input": 1.00, "output": 5.00},
       "claude-opus-4-5": {"input": 5.00, "output": 25.00},
   }
   ```

2. **Token estimation method:**
   ```python
   def estimate_tokens(self, text: str) -> dict:
       """Count tokens and estimate cost before extraction.

       Returns dict with:
         - input_tokens: int (from API)
         - estimated_output_tokens: int (conservative estimate)
         - estimated_cost_usd: float
         - model: str
       """
   ```

   Implementation:
   - Use client.messages.count_tokens() API (free call)
   - Pass model=self.model, system=self.SYSTEM_PROMPT, messages=[{"role": "user", "content": text}]
   - Calculate estimated_output = min(input_tokens // 4, 4096) (extraction typically <25% of input)
   - Calculate cost using _PRICING dict
   - Return all info as dict

3. **Cost calculation helper:**
   ```python
   def _calculate_cost(self, input_tokens: int, output_tokens: int) -> float:
       """Calculate API cost in USD."""
       prices = self._PRICING.get(self.model, self._PRICING["claude-sonnet-4-5"])
       input_cost = (input_tokens / 1_000_000) * prices["input"]
       output_cost = (output_tokens / 1_000_000) * prices["output"]
       return input_cost + output_cost
   ```

4. **For chunked documents:**
   - estimate_tokens() should accept optional `chunks: list[str]` parameter
   - Sum token counts across all chunks
   - Multiply output estimate by chunk count
  </action>
  <verify>
    - `ruff check src/cosmograph/extractors/llm.py` passes
    - Method signature: `estimate_tokens(self, text: str, chunks: list[str] | None = None) -> dict`
    - Cost calculation matches research formula: $3/MTok input, $15/MTok output for Sonnet 4.5
  </verify>
  <done>
    LlmExtractor has estimate_tokens() method that returns token counts and cost estimate.
    Uses official token counting API (free call, accurate counts).
  </done>
</task>

<task type="auto">
  <name>Task 2: Add approval gate with Rich display</name>
  <files>
    src/cosmograph/extractors/llm.py
  </files>
  <action>
Add data sovereignty approval gate to LlmExtractor:

1. **Add constructor parameter for interactive mode:**
   ```python
   def __init__(
       self,
       graph: Graph | None = None,
       model: str = "claude-sonnet-4-5",
       interactive: bool = True,  # NEW
   ):
       self.interactive = interactive
   ```

2. **Approval gate method:**
   ```python
   def _approval_gate(self, text: str, estimate: dict) -> bool:
       """Display extraction details and get operator approval.

       Returns True if approved, False if declined.
       For non-interactive mode, always returns True.
       """
   ```

   Implementation:
   - If not self.interactive: return True
   - Import Rich: `from rich.console import Console` and `from rich.table import Table`
   - Create console = Console()
   - Generate document hash: hashlib.sha256(text.encode()).hexdigest()[:16]
   - Display Rich table with:
     - Document Hash (for audit, not content)
     - Input Tokens (formatted with commas)
     - Est. Output Tokens
     - Est. Cost (USD) formatted as $X.XXXX
     - Model name
   - Print warning: "[yellow]This will send document content to Anthropic's Claude API.[/yellow]"
   - Prompt: console.input("[bold]Proceed with extraction? [y/N]: [/bold]")
   - Return response.lower() in ('y', 'yes')

3. **Integrate into extract() method:**
   - Before any API calls, calculate estimate for full document
   - Call _approval_gate(text, estimate)
   - If declined, raise OperatorDeclinedError (new exception class)
   - If approved, proceed with extraction

4. **Create OperatorDeclinedError exception:**
   ```python
   class OperatorDeclinedError(Exception):
       """Raised when operator declines LLM extraction."""
       pass
   ```

5. **Audit logging:**
   - After approval, log: f"LLM extraction approved for doc {doc_hash}, {input_tokens} tokens"
   - Use standard logging module at INFO level
   - Never log document content
  </action>
  <verify>
    - `ruff check src/cosmograph/extractors/llm.py` passes
    - OperatorDeclinedError is importable
    - Interactive mode shows Rich table and prompts (manual verification)
    - Non-interactive mode skips prompt
  </verify>
  <done>
    Approval gate shows token estimate and cost before extraction.
    Operator must type 'y' to proceed (or use non-interactive mode).
    Document hash logged for audit trail.
  </done>
</task>

</tasks>

<verification>
After both tasks:
1. `ruff check src/cosmograph/extractors/llm.py` passes
2. LlmExtractor has methods: estimate_tokens, _approval_gate, _calculate_cost
3. OperatorDeclinedError exception exists
4. Constructor accepts `interactive: bool` parameter
5. Pricing constants match research document
</verification>

<success_criteria>
- Token counting uses official API (not character estimation)
- Cost estimate displayed with model-specific pricing
- Rich table shows: doc hash, input tokens, estimated output, cost
- Yellow warning about sending content to external API
- Interactive prompt requires explicit 'y' to proceed
- Non-interactive mode bypasses prompt
- Operator declined raises OperatorDeclinedError
- Document hash (not content) logged for audit
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-extractor/04-02-SUMMARY.md`
</output>
