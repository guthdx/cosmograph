---
phase: 04-llm-extractor
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cosmograph/extractors/llm.py
  - src/cosmograph/extractors/__init__.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "LlmExtractor can call Claude API with a document"
    - "Structured output returns entities and relationships"
    - "Response is parsed into Graph nodes/edges"
    - "Large documents are chunked appropriately"
    - "Rate limits are handled with retries"
  artifacts:
    - path: "src/cosmograph/extractors/llm.py"
      provides: "LlmExtractor class with extraction and chunking"
      min_lines: 200
    - path: "pyproject.toml"
      provides: "Updated llm dependencies"
      contains: "anthropic>=0.76.0"
  key_links:
    - from: "src/cosmograph/extractors/llm.py"
      to: "anthropic.Anthropic"
      via: "SDK client instantiation"
      pattern: "anthropic\\.Anthropic"
    - from: "src/cosmograph/extractors/llm.py"
      to: "models.Graph"
      via: "BaseExtractor inheritance"
      pattern: "class LlmExtractor\\(BaseExtractor\\)"
---

<objective>
Create the core LlmExtractor class that uses Claude API with structured outputs to extract entities and relationships from documents.

Purpose: Enable intelligent extraction that understands semantic relationships beyond what regex patterns can capture.

Output: Working LlmExtractor with chunking, structured output parsing, and rate limit handling.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/milestones/v0.2.0/ROADMAP.md
@.planning/phases/04-llm-extractor/04-RESEARCH.md

@src/cosmograph/extractors/base.py
@src/cosmograph/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update dependencies and create LlmExtractor with Pydantic schemas</name>
  <files>
    pyproject.toml
    src/cosmograph/extractors/llm.py
  </files>
  <action>
1. Update pyproject.toml [llm] optional dependencies:
   - Change `anthropic>=0.18.0` to `anthropic>=0.76.0` (needed for token counting API)
   - Add `tenacity>=8.2.0` (rate limit handling)
   - Remove openai (not used)
   - Add `python-dotenv>=1.0.0` (credential management)

2. Create `src/cosmograph/extractors/llm.py` with:

   **Pydantic schemas for structured output:**
   ```python
   class ExtractedEntity(BaseModel):
       id: str
       name: str
       category: str
       description: str = ""

   class ExtractedRelationship(BaseModel):
       source_id: str
       target_id: str
       relationship_type: str

   class ExtractionResult(BaseModel):
       entities: list[ExtractedEntity]
       relationships: list[ExtractedRelationship]
   ```

   **LlmExtractor class:**
   - Inherit from BaseExtractor
   - Constructor accepts `model: str = "claude-sonnet-4-5"` parameter
   - Store system prompt as class constant SYSTEM_PROMPT
   - Implement `supports()` returning True for .txt, .md, .pdf files
   - Implement `extract()` that:
     a. Reads document text
     b. Chunks if necessary (use _chunk_document helper)
     c. For each chunk, calls _extract_chunk()
     d. Merges results into self.graph

   **Chunking logic (_chunk_document):**
   - Max 100,000 tokens per chunk (~400,000 chars)
   - 500 token overlap (~2,000 chars) for context continuity
   - Split at paragraph boundaries (\n\n) when possible
   - Return list of chunk strings

   **Extraction prompt (SYSTEM_PROMPT):**
   Design prompt that instructs Claude to extract:
   - Entities: people, organizations, government bodies, legal concepts, documents, locations, dates
   - Relationships: defines, references, amends, supersedes, establishes, governs, authorizes, belongs_to, contains
   - Be thorough but precise - only extract clearly stated facts

   **Response parsing (_parse_result):**
   - Convert ExtractionResult to Graph nodes/edges
   - Use graph.add_node() for each entity (category from entity.category)
   - Use graph.add_edge() for each relationship
   - Handle deduplication (Graph class handles this)

   **Rate limiting:**
   - Use tenacity decorator on _call_api method
   - wait_random_exponential(min=1, max=60)
   - stop_after_attempt(6)
   - retry_if_exception_type(RateLimitError)
  </action>
  <verify>
    - `ruff check src/cosmograph/extractors/llm.py` passes
    - `mypy src/cosmograph/extractors/llm.py` passes (or only expected SDK issues)
    - File imports correctly: `python -c "from cosmograph.extractors.llm import LlmExtractor"`
  </verify>
  <done>
    LlmExtractor class exists with structured output schemas, chunking, and rate limit handling.
    Dependencies updated in pyproject.toml.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire up API calls with structured outputs</name>
  <files>
    src/cosmograph/extractors/llm.py
  </files>
  <action>
Implement the actual Claude API integration in LlmExtractor:

1. **Client initialization:**
   - In __init__, create self.client = anthropic.Anthropic() (uses ANTHROPIC_API_KEY env var)
   - Add error handling if key missing: raise ValueError with helpful message

2. **_extract_chunk method:**
   ```python
   def _extract_chunk(self, chunk: str) -> ExtractionResult:
       """Extract entities and relationships from a text chunk."""
       response = self._call_api(chunk)
       return response.parsed_output
   ```

3. **_call_api method with rate limiting:**
   - Use client.beta.messages.parse() for structured outputs
   - Pass model=self.model
   - Pass betas=["structured-outputs-2025-11-13"]
   - Pass max_tokens=4096 (reasonable for extraction output)
   - Pass output_format=ExtractionResult
   - Pass system=self.SYSTEM_PROMPT
   - Pass messages=[{"role": "user", "content": chunk}]

4. **Handle potential refusals:**
   - Check response.stop_reason
   - If "refusal", log warning and return empty ExtractionResult
   - This handles edge cases where Claude declines extraction

5. **Graceful degradation:**
   - If anthropic not installed, raise ImportError with install instructions
   - Wrap import in try/except at module level:
     ```python
     try:
         import anthropic
         from anthropic import RateLimitError
         HAS_ANTHROPIC = True
     except ImportError:
         HAS_ANTHROPIC = False
         anthropic = None
         RateLimitError = Exception
     ```

6. **Export from __init__.py:**
   - Add conditional import: `if HAS_ANTHROPIC: from .llm import LlmExtractor`
   - Or always import but check HAS_ANTHROPIC at runtime
  </action>
  <verify>
    - `ruff check src/cosmograph/extractors/llm.py` passes
    - `python -c "from cosmograph.extractors.llm import LlmExtractor, HAS_ANTHROPIC; print(HAS_ANTHROPIC)"` prints True/False
    - Code structure matches patterns from 04-RESEARCH.md
  </verify>
  <done>
    LlmExtractor can make Claude API calls with structured outputs.
    Graceful degradation when anthropic not installed.
    Rate limiting with tenacity retry decorator.
  </done>
</task>

</tasks>

<verification>
After both tasks:
1. `pip install -e ".[llm,dev]"` succeeds (installs anthropic>=0.76.0, tenacity)
2. `ruff check src/cosmograph/extractors/` passes
3. `from cosmograph.extractors.llm import LlmExtractor` works
4. LlmExtractor has methods: extract, supports, _chunk_document, _extract_chunk, _call_api, _parse_result
</verification>

<success_criteria>
- LlmExtractor class inherits BaseExtractor and implements extract()/supports()
- Pydantic schemas defined for structured extraction output
- Document chunking splits at paragraph boundaries with overlap
- Rate limiting decorator handles RateLimitError with exponential backoff
- API calls use structured outputs beta feature
- Module gracefully handles missing anthropic installation
</success_criteria>

<output>
After completion, create `.planning/phases/04-llm-extractor/04-01-SUMMARY.md`
</output>
