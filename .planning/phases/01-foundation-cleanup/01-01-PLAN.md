---
phase: 01-foundation-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/test_models.py
  - tests/fixtures/sample_constitution.txt
  - tests/fixtures/sample_code.txt
autonomous: true

must_haves:
  truths:
    - "pytest discovers and runs model tests"
    - "Graph._clean_id() behavior verified by tests"
    - "Graph.add_node() deduplication verified"
    - "Graph.add_edge() self-loop prevention verified"
    - "Node and Edge to_dict() output verified"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Shared pytest fixtures"
      contains: "@pytest.fixture"
    - path: "tests/test_models.py"
      provides: "Unit tests for Graph, Node, Edge"
      min_lines: 80
    - path: "tests/fixtures/sample_constitution.txt"
      provides: "Test data for extractor tests"
  key_links:
    - from: "tests/test_models.py"
      to: "src/cosmograph/models.py"
      via: "import Graph, Node, Edge"
      pattern: "from cosmograph.models import"
---

<objective>
Add comprehensive unit tests for core data models (Graph, Node, Edge).

Purpose: Establish test coverage as safety net before refactoring models.py and extractors. Tests must pass before and after changes in subsequent plans.

Output: tests/conftest.py, tests/test_models.py, tests/fixtures/ directory with sample documents
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/milestones/v0.2.0/ROADMAP.md
@.planning/phases/01-foundation-cleanup/01-RESEARCH.md

# Source files to test
@src/cosmograph/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create pytest fixtures and test data</name>
  <files>tests/conftest.py, tests/fixtures/sample_constitution.txt, tests/fixtures/sample_code.txt</files>
  <action>
Create tests/conftest.py with shared fixtures:
- `empty_graph`: Fresh Graph instance for each test
- `graph_with_nodes`: Graph with 3 sample nodes (node1, node2, node3 of category "test")
- `sample_node`: Single Node instance for unit tests
- `sample_edge`: Single Edge instance for unit tests

Create tests/fixtures/ directory with sample test documents:
- sample_constitution.txt: ~20 lines with ARTICLE, SECTION patterns for extractor tests later
- sample_code.txt: ~20 lines with TITLE, CHAPTER patterns

Use function-scoped fixtures (pytest default) to ensure test isolation.
  </action>
  <verify>
python -c "from tests.conftest import *; print('Fixtures import OK')"
ls tests/fixtures/
  </verify>
  <done>conftest.py exists with 4 fixtures, fixtures/ has 2 sample files</done>
</task>

<task type="auto">
  <name>Task 2: Add unit tests for Graph, Node, Edge</name>
  <files>tests/test_models.py</files>
  <action>
Create tests/test_models.py with test classes:

**TestNode:**
- test_to_dict_truncates_label: Verify label truncated to 60 chars
- test_to_dict_truncates_description: Verify description truncated to 150 chars
- test_to_dict_structure: Verify dict has id, label, category, description keys

**TestEdge:**
- test_to_dict_structure: Verify dict has source, target, type keys
- test_edge_type_in_dict: Verify edge_type maps to "type" key

**TestGraphCleanId:**
- test_removes_special_characters: "Test!@#$Node" -> "TestNode"
- test_collapses_whitespace: "Test   Node" -> "Test Node"
- test_truncates_to_100_chars: 150 chars input -> 100 char output
- test_handles_empty_string: "" -> ""

**TestGraphAddNode:**
- test_adds_new_node: Returns clean ID, node in graph.nodes
- test_deduplicates_nodes: Same ID returns existing, doesn't duplicate
- test_returns_clean_id: ID cleaning applied to return value

**TestGraphAddEdge:**
- test_adds_edge_successfully: Returns True, edge in graph.edges
- test_prevents_self_loops: Same source/target returns False
- test_prevents_duplicate_edges: Same edge twice returns False second time
- test_allows_different_edge_types: Same nodes, different type allowed

**TestGraphStats:**
- test_get_stats_counts: Verify node/edge counts accurate
- test_get_stats_categories: Verify category breakdown correct

Use pytest assertions. Import from cosmograph.models. Use fixtures from conftest.py.
  </action>
  <verify>
cd /Users/guthdx/terminal_projects/cosmograph && source .venv/bin/activate && pytest tests/test_models.py -v
  </verify>
  <done>All tests pass, coverage >80% on models.py (verify with pytest --cov=src/cosmograph/models tests/test_models.py)</done>
</task>

</tasks>

<verification>
```bash
# Run tests with coverage
cd /Users/guthdx/terminal_projects/cosmograph
source .venv/bin/activate
pytest tests/test_models.py -v --cov=src/cosmograph/models --cov-report=term-missing

# Verify coverage threshold
pytest tests/test_models.py --cov=src/cosmograph/models --cov-fail-under=80
```
</verification>

<success_criteria>
- pytest discovers and runs all model tests
- All tests pass (green)
- Coverage on models.py exceeds 80%
- Tests are isolated (run in any order)
- Fixtures properly scoped (no state leakage)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation-cleanup/01-01-SUMMARY.md`
</output>
